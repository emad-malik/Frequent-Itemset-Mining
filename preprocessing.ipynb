{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the data set (1 GB sample size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the sampled dataset: 102562\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the filename of the sampled dataset in the workspace\n",
    "sampled_dataset_filename = \"sample_1gb_dataset.json\" \n",
    "\n",
    "# Function to load the sampled dataset\n",
    "def load_sampled_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Parse each line as JSON and append to the dataset list\n",
    "            dataset.append(json.loads(line))\n",
    "    return dataset\n",
    "\n",
    "# Load the sampled dataset\n",
    "sampled_dataset = load_sampled_dataset(sampled_dataset_filename)\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(\"Number of records in the sampled dataset:\", len(sampled_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre processing with BATCH PROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Preprocessed dataset saved as: preprocessed_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the filename for the preprocessed dataset\n",
    "preprocessed_dataset_filename = \"preprocessed_dataset.json\"\n",
    "\n",
    "# Function to write the preprocessed dataset to a JSON file in the workspace\n",
    "def write_preprocessed_dataset(dataset, filename):\n",
    "    with open(filename, 'a', encoding='utf-8') as f:  # Use 'a' mode for appending to the file\n",
    "        for record in dataset:\n",
    "            # Serialize each record to JSON and write it to the file\n",
    "            json.dump(record, f)\n",
    "            f.write('\\n')  # Add a newline character after each record\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to clean and format the dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    cleaned_dataset = []\n",
    "    for record in dataset:\n",
    "        # Convert data types and handle missing values\n",
    "        cleaned_record = {\n",
    "            \"asin\": record.get(\"asin\", \"\"),\n",
    "            \"title\": record.get(\"title\", \"\"),\n",
    "            \"feature\": record.get(\"feature\", []),\n",
    "            \"description\": record.get(\"description\", \"\"),\n",
    "            \"price\": convert_to_float(record.get(\"price\", \"\")),\n",
    "            \"imageURL\": record.get(\"imageURL\", \"\"),\n",
    "            \"highResolutionImageURL\": record.get(\"highResolutionImageURL\", \"\"),\n",
    "            \"related\": record.get(\"related\", {}),\n",
    "            \"salesRank\": record.get(\"salesRank\", {}),\n",
    "            \"brand\": record.get(\"brand\", \"\"),\n",
    "            \"categories\": record.get(\"categories\", []),\n",
    "            \"tech1\": record.get(\"tech1\", {}),\n",
    "            \"tech2\": record.get(\"tech2\", {}),\n",
    "            \"similar\": record.get(\"similar\", {}),\n",
    "        }\n",
    "        cleaned_dataset.append(cleaned_record)\n",
    "    return cleaned_dataset\n",
    "\n",
    "#BATCH PROCESSING (done for 15 gb wala sample size)\n",
    "# Define chunk size (adjust as needed)\n",
    "chunk_size = 1000\n",
    "\n",
    "# Load the sampled dataset\n",
    "with open(sampled_dataset_filename, 'r', encoding='utf-8') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            # Read a chunk of data\n",
    "            chunk = [json.loads(next(f)) for _ in range(chunk_size)]\n",
    "            # Preprocess the chunk\n",
    "            preprocessed_chunk = preprocess_dataset(chunk)\n",
    "            # Write the preprocessed chunk to the output file\n",
    "            write_preprocessed_dataset(preprocessed_chunk, preprocessed_dataset_filename)\n",
    "        except StopIteration:\n",
    "            break  # Break the loop if no more data is available\n",
    "\n",
    "print(\"Preprocessing completed. Preprocessed dataset saved as:\", preprocessed_dataset_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
